# Outputs

Dataset files generated by the filtering pipeline. These are gitignored due to size.

## Files

### `filtered_dataset.jsonl`
Full filtered dataset with model evaluations.

- **Source**: `siro1/kernelbook-glm4-evals`
- **Samples**: 7,181 (filtered from 18,162 where reward > 0.85)
- **HuggingFace**: https://huggingface.co/datasets/siro1/kernelbook-glm4-evals-filtered

**Columns (15 total):**
- Original (12): `example_id`, `prompt`, `completion`, `task`, `reward`, `generation_ms`, `scoring_ms`, `total_ms`, `info`, `answer`, `speedup_reward`, `oai_tools`
- Added (3): `difficulty`, `style`, `evaluation_raw`

### `filtered_dataset-filtered.jsonl`
Deduplicated dataset (unique by module name).

- **Samples**: 2,967 (unique modules from 7,181)
- **HuggingFace**: https://huggingface.co/datasets/siro1/kernelbook-glm4-evals-unique

## Statistics

### Difficulty Distribution (0-10 scale)
```
0:     1    1:   513    2: 1,119    3:   636    4: 1,830
5: 1,154    6: 1,260    7:   406    8:   116    9:    19
Mean: 4.15, Std: 1.76
```

### Style Distribution (0-10 scale)
```
0:   559    1: 4,167    2: 1,351    3:   328    4:   112
5:   184    6:   330    7:     9
Mean: 1.60, Std: 1.37
```

### Reward Statistics
- Mean: 1.006, Std: 0.050
- Range: [0.851, 2.097]

## Reproduction

```bash
# Generate filtered dataset
uv run python scripts/filter_dataset.py

# Generate unique dataset
uv run python scripts/keep_best.py
```

## Upload to HuggingFace

```python
from datasets import Dataset
from huggingface_hub import login
import json

login(token="your_token")

data = [json.loads(l) for l in open("outputs/filtered_dataset.jsonl")]
Dataset.from_list(data).push_to_hub("your-username/dataset-name")
```
